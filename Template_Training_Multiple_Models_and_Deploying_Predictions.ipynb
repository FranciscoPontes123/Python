{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Possibly relevant libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RepeatedStratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error, f1_score, classification_report, accuracy_score, recall_score, precision_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific template, we are attempting to predict the classification for the test dataset, adapt the code according to your necessities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = #train_dataset\n",
    "X_val = #val_dataset\n",
    "X_test = #test_dataset\n",
    "y_train = #y_train_dataset\n",
    "y_val = #y_val_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation of Multiple Models at once ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=200),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Bagging\": BaggingClassifier(random_state=42),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    \"GBM\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGB\":XGBClassifier(random_state=42),\n",
    "    \"Neural Network\": MLPClassifier(random_state=42)\n",
    "}\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_val)\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'f1_macro': f1_score(y_val, y_pred, average='macro'),\n",
    "        'accuracy': accuracy_score(y_val, y_pred)\n",
    "    }\n",
    "    #Adapt the metrics you want to use for evaluation\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"{model_name}: F1 macro = {metrics['f1_macro']:.4f}, Accuracy = {metrics['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Predictions (Useful for Kaggle Submissions) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for name, model in models.items():\n",
    "        print(f\"Predicting with {name}...\")\n",
    "        predictions[name] = model.predict(X_test)  # Predict on df_test\n",
    "    \n",
    "for name, preds in predictions.items():\n",
    "    preds = [[pred] for pred in preds] \n",
    "    predictions_df = pd.DataFrame({\n",
    "        'id': X_test.index,\n",
    "        'prediction': preds\n",
    "    })\n",
    "\n",
    "    predictions_df.to_csv(f'{name}_predictions.csv', index=False)\n",
    "    print(f\"Predictions for {name} saved to '{name}_predictions.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
